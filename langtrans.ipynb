{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053398 2282573\n",
      "116141 93919\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('hin2eng.csv', encoding='utf8')\n",
    "eng = df['english_sentence']\n",
    "hin = df['hindi_sentence']\n",
    "engWords = [word for sent in eng for word in str(sent).split()]\n",
    "hinWords = [word for sent in hin for word in str(sent).split()]\n",
    "print(len(engWords),len(hinWords))\n",
    "engWords=set(engWords)\n",
    "hinWords=set(hinWords)\n",
    "print(len(engWords),len(hinWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Help' 'Jump' 'Hello' 'Cheers' 'Got it'] ['बचाओ' 'उछलो' 'कूदो' 'छलांग' 'नमस्ते।']   English    Hindi\n",
      "0    Help     बचाओ\n",
      "1    Jump     उछलो\n",
      "2    Jump     कूदो\n",
      "3    Jump    छलांग\n",
      "4   Hello  नमस्ते।\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "def remove(text):\n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        data = str.maketrans('', '', string.punctuation)\n",
    "        return text.translate(data)\n",
    "    elif isinstance(text, float):\n",
    "        return \"nan\"\n",
    "    else:\n",
    "        return text\n",
    "df=pd.read_csv('eng2hin.csv',encoding='utf8')\n",
    "df['English']=df['English'].apply(remove)\n",
    "df['Hindi']=df['Hindi'].apply(remove)\n",
    "# print(df['English'].shape,df['Hindi'].shape)\n",
    "engWords=df['English'].unique()\n",
    "hinWords=df['Hindi'].unique()\n",
    "# print(engWords.shape,hinWords.shape,df.shape)\n",
    "print(engWords[:5],hinWords[:5],df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Tokenization and vocabulary building for English\u001b[39;00m\n\u001b[0;32m     13\u001b[0m eng_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43meng_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_on_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Tokenization and vocabulary building for Hindi\u001b[39;00m\n\u001b[0;32m     17\u001b[0m hin_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PAVAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\text.py:293\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    292\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 293\u001b[0m         seq \u001b[39m=\u001b[39m text_to_word_sequence(\n\u001b[0;32m    294\u001b[0m             text,\n\u001b[0;32m    295\u001b[0m             filters\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilters,\n\u001b[0;32m    296\u001b[0m             lower\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlower,\n\u001b[0;32m    297\u001b[0m             split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit,\n\u001b[0;32m    298\u001b[0m         )\n\u001b[0;32m    299\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m         seq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manalyzer(text)\n",
      "File \u001b[1;32mc:\\Users\\PAVAN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\preprocessing\\text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[39mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39m    A list of words (or tokens).\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 74\u001b[0m     input_text \u001b[39m=\u001b[39m input_text\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     76\u001b[0m translate_dict \u001b[39m=\u001b[39m {c: split \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m filters}\n\u001b[0;32m     77\u001b[0m translate_map \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('eng2hin.csv', encoding='utf8')\n",
    "\n",
    "# Train-validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Tokenization and vocabulary building for English\n",
    "eng_tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "eng_tokenizer.fit_on_texts(train_df['English'])\n",
    "\n",
    "# Tokenization and vocabulary building for Hindi\n",
    "hin_tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "hin_tokenizer.fit_on_texts(train_df['Hindi'])\n",
    "\n",
    "# Example: Convert sentences to sequences\n",
    "train_eng_sequences = eng_tokenizer.texts_to_sequences(train_df['English'])\n",
    "train_hin_sequences = hin_tokenizer.texts_to_sequences(train_df['Hindi'])\n",
    "\n",
    "# Example: Pad sequences\n",
    "max_eng_length = 50  # Adjust based on your data\n",
    "max_hin_length = 60  # Adjust based on your data\n",
    "\n",
    "padded_train_eng_sequences = pad_sequences(train_eng_sequences, maxlen=max_eng_length, padding='post')\n",
    "padded_train_hin_sequences = pad_sequences(train_hin_sequences, maxlen=max_hin_length, padding='post')\n",
    "\n",
    "# Vocabulary sizes\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8149cf2560aa4c8b192dd9d399cb96214c9fa2bcf75f89691385e61b9df1d77d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
